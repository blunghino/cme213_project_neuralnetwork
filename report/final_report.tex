\documentclass[12pt]{article}

\usepackage{graphicx}

\usepackage[margin=0.75in]{geometry}

% for \FloatBarrier to keep figs in sections
\usepackage[section]{placeins}

% C++ Code
\usepackage{alltt, listings, textcomp, color, verbatim}

\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{dkgreen}{rgb}{0,0.4,0}
\definecolor{mylilas}{RGB}{170,55,241}
\definecolor{dkblue}{rgb}{0.0,0.0,0.4}

\lstdefinestyle{cpp}{
	language=C++,                        % choose the language of the code
	basicstyle=\ttfamily\footnotesize,   % the size of the fonts that are used for the code
	tabsize=2,                           % sets default tabsize to 2 spaces
	showstringspaces=false,              % show spaces adding particular underscores
	showtabs=false,                      % show tabs within strings adding particular underscores
	keywordstyle=\color{blue},           % keyword style
	identifierstyle=\color{black},       % identifier style
	emphstyle=\color{black}\bf,          % emphasis style
	commentstyle=\color{dkgreen}\slshape,% comment style
	stringstyle=\color{mylilas},         % string literal style
	aboveskip=\baselineskip,             % skip space when starting code environment
	xleftmargin=10pt, xrightmargin=10pt, % code margins
	frame=lines,                         % adds a frame around the code
	numbers=left,                        % where to put the line-numbers
	numberstyle=\tiny,                   % the size of the fonts that are used for the line-numbers
	numbersep=10pt,                      % how far the line-numbers are from the code
}
\lstnewenvironment{cpp}{\lstset{style=cpp}}{}
\newcommand{\inputcpp}[1]{\lstinputlisting[style=cpp]{#1}}

\title{CME 213 Project Final Report}	
\author{Brent Lunghino\\lunghino@stanford.edu}

\begin{document}
	
\maketitle

\section{Summary}

In this report I analyze the performance and correctness of my neural network. My final network is the result of 4 iterations of profiling and performance improvements. In each section of the report I describe the features of my implementation, identify performance bottlenecks, and discuss strategies for improving performance.

\section{Implementation 1}

My first correct implementation laid a sound foundation to build on. This implementation established the pattern of communication between MPI nodes and between CPU and GPU. For each batch the training set data is divided among MPI nodes. On each node the training data and the current set of network coefficients is copied to the GPU at the beginning of the feed-forward process. The data remains on the GEMM kernels for all feed-forward and backpropagation steps  uses "naive" gpu kernels to perform calculations, with each process being launched by 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=.7\linewidth]{fig/pie_chart.png}
		\caption{Relative contribution of top three processes to computation time.}
		\label{fig:pie}
	\end{center}
\end{figure}

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=\linewidth]{fig/screenshot1.png}
		\caption{NVVP visualization of run chonology for Implementation 1.}
		\label{fig:screenshot}
	\end{center}
\end{figure}
\FloatBarrier

\section{Correctness Analysis}

Round off errors increase with learning rate and 

\end{document}