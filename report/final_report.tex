\documentclass[12pt]{article}

\usepackage{graphicx}

\usepackage[margin=0.75in]{geometry}

% for \FloatBarrier to keep figs in sections
\usepackage[section]{placeins}

% C++ Code
\usepackage{alltt, listings, textcomp, color, verbatim}

\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{dkgreen}{rgb}{0,0.4,0}
\definecolor{mylilas}{RGB}{170,55,241}
\definecolor{dkblue}{rgb}{0.0,0.0,0.4}

\lstdefinestyle{cpp}{
	language=C++,                        % choose the language of the code
	basicstyle=\ttfamily\footnotesize,   % the size of the fonts that are used for the code
	tabsize=2,                           % sets default tabsize to 2 spaces
	showstringspaces=false,              % show spaces adding particular underscores
	showtabs=false,                      % show tabs within strings adding particular underscores
	keywordstyle=\color{blue},           % keyword style
	identifierstyle=\color{black},       % identifier style
	emphstyle=\color{black}\bf,          % emphasis style
	commentstyle=\color{dkgreen}\slshape,% comment style
	stringstyle=\color{mylilas},         % string literal style
	aboveskip=\baselineskip,             % skip space when starting code environment
	xleftmargin=10pt, xrightmargin=10pt, % code margins
	frame=lines,                         % adds a frame around the code
	numbers=left,                        % where to put the line-numbers
	numberstyle=\tiny,                   % the size of the fonts that are used for the line-numbers
	numbersep=10pt,                      % how far the line-numbers are from the code
}
\lstnewenvironment{cpp}{\lstset{style=cpp}}{}
\newcommand{\inputcpp}[1]{\lstinputlisting[style=cpp]{#1}}

\title{CME 213 Project Final Report}	
\author{Brent Lunghino\\lunghino@stanford.edu}

\begin{document}
	
\maketitle

\section{Summary}

In this report I analyze the performance and correctness of my neural network. My final network is the result of 4 iterations of profiling and performance improvements. In each section of the report I describe the features of my implementation, identify performance bottlenecks, and discuss strategies for improving performance.

\section{Implementation 1}

My first correct implementation laid a sound foundation to build on. This implementation established the pattern of communication between MPI nodes and between CPU and GPU. For each batch the training set data is divided among MPI nodes. On each node the training data and the current set of network coefficients is copied to the GPU at the beginning of the feed-forward process. The data remains on the GEMM kernels for all feed-forward and backpropagation steps. The computed gradients are then copied back to the CPU on each MPI node. The gradients are summed on each MPI node and then used to update the network coefficients, so each MPI node has an identical set of network coefficients. The structure of communications described above is fast because it minimizes the amount of data transferred between CPU and GPU using cudaMemcpy and also minimizes the amount of data transferred between MPI nodes .

This implementation uses very simple GEMM kernels, with each entry in the result matrix being computed by an indvidual thread and each thread loading all data needed for the calculation from global memory. Profiling the implementation unsurprisingly shows that 99\% of the compute time is spent in the GEMM kernels (Figure~\ref{fig:screenshot1}). The GEMM kernels are slow because each thread is performing many uncoalesced reads from global memory. A first step to reducing compute time would be to leverage shared memory to reduce reads from global memory.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=\linewidth]{fig/screenshot1.png}
		\caption{NVVP visualization of run chonology for Implementation 1.}
		\label{fig:screenshot1}
	\end{center}
\end{figure}

\section{Implementation 2}

This implementation features an improved GEMM kernel that loads square blocks of data from the input matrices into shared memory. Each thread still computes a single value in the result matrix. The data in shared memory is used by a thread block to update the result in 

\section{Correctness Analysis}

Round off errors increase with learning rate and 

\end{document}